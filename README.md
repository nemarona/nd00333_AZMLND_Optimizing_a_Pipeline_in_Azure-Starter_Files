# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
This dataset contains anonymized socio-economic information for 32,950 individuals,
including age, job, marital status, etc., for a total of 20 categorical and numerical variables.
We seek to predict the value of a variable called `y` that takes yes/no values only
and has a highly unbalanced distribution, with yes/no = 3692/29,258.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a voting ensemble found by AutoML, which yielded an accuracy of 91.7%.
The simple logistic regression classifier used by HyperDrive came in very close, with an accuracy of 91.1%

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Data preparation for the scikit-learn pipeline includes one-hot encoding of categorical variables and coding months and days with integer values.
The data is split in train and tests datasets, and the train dataset is used to train a logistic regression estimator
with two hyperparameters: inverse regularization strength, `C` and maximum number of iterations, `max_iter`.
We use HyperDrive to tune both hyperparameters.
For `C` we use a uniformly distributed random variable between 0.1 and 2.0
while for `max_iter` we choose between a fixed list of possible values
(10, 20, 50, 100, 200).
These values seem like sensible choices because `C` must be a positive float,
with smaller values leading to stronger regularization,
and `max_iter` must be an integer.

Best model:
- ID: HD_7080e9fb-0533-4e65-bdd3-749c6440fd33_3
- Accuracy: 0.9113863
- Max iterations: 20
- Regularization Strength: 1.854653

**What are the benefits of the parameter sampler you chose?**
Random sampling supports discrete and continuous hyperparameters as well as early termination of low-performance jobs.
It can be used as an initial search before refining the search space to improve results.


**What are the benefits of the early stopping policy you chose?**
Bandit policy is based on slack factor/slack amount and evaluation interval. 
It ends a job when the primary metric isn't within the specified slack factor/slack amount of the most successful job.
This means that no time is wasted on jobs that show very little promise.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML's best model was a voting ensemble of six models, including several XGBoost classifiers, with weights equal to either 1/7 or 2/7.
Accuracy was 0.91724.


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
AutoML's best model is much more complex than the logistic regression classifier used by HyperDrive.
The difference in accuracy was minimal; this may be due to the fact that the problem is relatively simple.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
With an accuracy of nearly 92%, there is little room for improvement.
Given the stark class imbalance in the target variable, it may prove worthwhile to check whether other metrics beside accuracy are more relevant for this data, and to use them for optimization.
